---
title: "Final Project"
author: "Filip Tomaska, Kelli Woodward"
subtitle: PSTAT 131/231
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: yes
  pdf_document:
    toc: yes
bibliography: references.bib
---

# Abstract

Neuronal activity can be recorded using various recording techniques. Optophysiology provides an option using genetically encoded fluorescent reporters, which generate optical readout of neuronal activity.

# Introduction

## Theoretical rationale

The classical view of the auditory cortex as a set of linear filter units has been, among else, motivated by its tonotopic organization. Single neurons were regarded as linear filters with receptive fields constructed by using artificial single modality changes. However, advancements enabling recordings of large local populations resulted in data suggesting the original hypothesis might have been seriously influenced by the nature of the experiments. Models based on linear receptive fields of single cells in response to largely artificial simple sounds have failed to predict responses of these cells to more complex stimuli. Based on the analysis of population responses to simple and complex stimuli, it has been proposed the computations at the level of the auditory cortex are based on perceptual categories exhibiting attractor-like dynamics. Abstract auditory objects represent a robust coding strategy more in line with the large nonlinearities observed when comparing single cell responses based on receptive fields with natural stimuli such as vocalizations. Arguably, features of a basic coding strategy should include a reasonable degree of stability in time. While the notion of single filter units organized into maps has been supported by longitudinal studies, the temporal stability of patterns representing perceptual categories has not yet been thoroughly investigated. Previous reports have suggested that population responses exhibiting inter-trial stability within a single session of an acute experiment are behaviorally relevant. We aim to test whether the degree of stability of activity patterns over 30 days is comparable to that observed within a single session

```{r library loading, error=F, message=FALSE, warning=FALSE, echo=T}
library(R.matlab)
library(readr)
library(tidyverse)
library(data.table)
library(cluster)
library(dendextend)
library(factoextra)
library(ROCR)
library(boot)
library(caret)
library(glmnet) 
library(tree) 
library(maptree) 
library(randomForest) 
library(gbm) 
```

# Pre-processing

## Data loading

Some of the functional analysis had to be performed in MATLAB since the Two Photon Processor [@tomek2013] , the framework used for raw fluorescence trace analysis, does not exist outside MATLAB. Consequently, data in the form of a MATLAB structured list is imported and transformed into a tibble dataframe

```{r data import, error=F, warning=FALSE, echo=T, cache=TRUE}

dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f19<-dataMat$sortAudFile[1]
f33<-dataMat$sortAudFile[2]
f34a<-dataMat$sortAudFile[3]
f34b <-dataMat$sortAudFile[4]
f34c <-dataMat$sortAudFile[5]
f35  <-dataMat$sortAudFile[6]
f37  <-dataMat$sortAudFile[7]
f38 <-dataMat$sortAudFile[8]
f40 <-dataMat$sortAudFile[9]
f41a <-dataMat$sortAudFile[10]
f41b <-dataMat$sortAudFile[11]
f42  <-dataMat$sortAudFile[12]
f43a <-dataMat$sortAudFile[13]
f43b <-dataMat$sortAudFile[14]
f44  <-dataMat$sortAudFile[15]

df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)

tempF = f34c[[1]]

for (j in c(1:8)){
  for (i in c(1:39)) {
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]),sound = i,day = j-1))
  }
  }
  
df$sound = factor(df$sound)
df$day = factor(df$day)
df.Day <-  df%>% filter(df$day==0)
df.neurons <-  df.Day%>% select(V1:V34)
```

# Exploratory

```{r, echo=T, warning=FALSE}

sumVect <- c(1:39)
  
for (i in c(1:39)) {
  df.sound = NULL
  df.sound <-  df%>% filter(df$sound==i)
  
  temp.df<- as.matrix(df.sound%>%select(V1:V34))

  sumVect[i] = sum(colSums(temp.df))
 
  sortSum = sort(sumVect, decreasing = TRUE)
  
  which(sumVect==(head(sortSum)[1]))
   
}
```

## hierarchical clustering

The hypothesis proposed by the work inspiring these experiments postulates that local neuronal populations represent sensory precepts by categorizing them into a small number of perceptual categories \[@bathellier2012\]. Since our aim was to reproduce these results using tools enabling long-term imaging, we first try to identify sounds producing a recognizable activity pattern.

In order to do this, we employ an unsupervised learning method, **hierarchical clustering**.

We perform this on a data subset from the first day of recordings, since that is when fields of view were selected for long term studies by performing clustering online during imaging.

### Randomized sampling

In order to assess the relevance of hierarchical clustering performed on our data, we first try to analyze a randomized dataset.

Finding clusters of similar activity patterns within a **randomized** (random sampling of sounds with replacement)dataset, might indicate that any clusters we observe in the actual data might be, in fact, an artifact arising from the nature of the data.

## Resampling to show data is not random

```{r,echo=T}
#reloading data

f34c = NULL
df = NULL
dataMat = NULL

dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f34c <-dataMat$sortAudFile[5]
df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)
tempF = f34c[[1]]

#random sampling, scrambling sound identity, with replacement

set.seed(123)

for (j in c(1:8)){
  for (i in c(2:39)) {
    sound.random = sample(c(1:39),1)
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]), sound = sound.random, day = j-1))
  }
  }
  
df$sound = factor(df$sound)
df$day = factor(df$day)
df.Day <-  df%>% filter(df$day==0)
df.neurons <-  df.Day%>% select(V1:V34)

#clustering
dist_mat <- dist(df.neurons,method = "euclidian")
hclust_avg <- hclust(dist_mat, method = "complete")
cut_avg <- cutree(hclust_avg, k = 2)
table(cut_avg, df.Day$sound)
```

```{r,echo=T}


table.accum <- matrix(, nrow=2,ncol=8)
maxSecond <- matrix(, ncol=8)

for (k in c(0:7)){
  
  df.Day <-  df%>% filter(df$day==k)


df.neurons <-  df.Day%>% select(V1:V34)
dist_mat <- dist(df.neurons,method = "euclidian")


hclust_avg <- hclust(dist_mat, method = "average")
cut_avg <- cutree(hclust_avg, k = 2)


table.accum[,k+1] = table(cut_avg, df.Day$sound)[,33]
maxSecond[k+1] = as.numeric(which.max(table(cut_avg, df.Day$sound)[2,]))

}

#Display the counts of 
table.accum
maxSecond
```

We can observe

```{r, error=F, warning=FALSE, echo=T, cache=TRUE}

#reloading data again for proper clustering

f34c = NULL
df = NULL


dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f34c <-dataMat$sortAudFile[5]

df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)

tempF = f34c[[1]]

for (j in c(1:8)){
  for (i in c(1:39)) {
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]),sound = i,day = j-1))
  }
  }
  
df$sound = factor(df$sound)
df$day = factor(df$day)
df.Day <-  df%>% filter(df$day==0)
df.neurons <-  df.Day%>% select(V1:V34)

```

### Looking at data day by day to determine which sound gets clustered in the second cluster the most

```{r,echo=T}


table.accum <- matrix(, nrow=2,ncol=8)
maxSecond <- matrix(, ncol=8)

for (k in c(0:7)){
  
  df.Day <-  df%>% filter(df$day==k)


df.neurons <-  df.Day%>% select(V1:V34)
dist_mat <- dist(df.neurons,method = "euclidian")


hclust_avg <- hclust(dist_mat, method = "average")
cut_avg <- cutree(hclust_avg, k = 2)


table.accum[,k+1] = table(cut_avg, df.Day$sound)[,33]
maxSecond[k+1] = as.numeric(which.max(table(cut_avg, df.Day$sound)[2,]))

}

#Display the counts of 
table.accum
maxSecond
```

#### Conclusion

The identities of sounds and their distribution are different in a randomized sample, thus our recordings are likely not an artifact.

## PCA analysis

In order to assess correlations between variables (neurons #1 to #38) we perform *principal component analysis*.

```{r, echo=T}

pca.neurons = prcomp(df.neurons, scale=TRUE, center = TRUE)

pr.var <- (pca.neurons$sdev)^2
pve=pr.var/sum(pr.var)

 plot(pve, xlab="Principal Component",
     ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
 
  plot(cumsum(pve), xlab="Principal Component ",
     ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')

abline(v =min(which((cumsum(pve)>=0.9))))

```

#### Conclusion PCA

We observe that in order to explain 90% of total variation in the data, we need at least **27** principal components. The high number of components suggests we indeed do need all of our variables in order to capture the variance, thus we should use all of them in our future analysis

# Supervised learning

## Data preparation

First, we need to define a factor variable defining whether the observation was obtained as a response to the examined sound. Based on the hierarchical clustering performed earlier, this sound is sound **33**,

```{r, include=FALSE}

df = df %>%
  mutate(s33=as.factor(ifelse(sound == "33", "Yes","No")))

```

#### Split

Since only one sound is considered a positive case, our data contains a small fraction of positive cases for learning and training purposes. For that reason, it is important to perform stratified sampling, so that the limited number of positive labels is evenly split between training and testing data chunks,

```{r, include=FALSE}

#stratified

set.seed(123)
df <- df %>% mutate(id = row_number())
#Check IDs
head(df$id)
train.index <- createDataPartition(df$s33, p = .7, list = FALSE)

train <- df[ train.index,]
test  <- df[-train.index,]
length(which(test$s33=="Yes"))
```

## Model fitting

### logistic regression

```{r, echo=T}
#generate subdatasets of only activity data for learning purposes
excl<-c('sound','day','id')
train.cv <- train %>% select(., -excl)
test.cv <- test %>% select(., -excl)
```

```{r, echo=T}
model.glm = glm(formula = s33~., data=train.cv, family = binomial)
#summary(model.glm)

```

```{r ,echo=T,message=FALSE,warning=FALSE}
calc_error_rate <- function(predicted.value, true.value){
  return(mean(true.value!=predicted.value)) }
```

```{r,echo=T,message=FALSE,warning=FALSE}
prob.training.glm = NULL
prob.training.glm = as.factor(ifelse(predict(model.glm, type="response")>0.5,"Yes","No"))

trainError<- calc_error_rate(prob.training.glm,train$s33)
trainError
testError<- calc_error_rate(prob.training.glm,test$s33)
testError



prob.training.glm = NULL
prob.training.glm =predict(model.glm)
pred.glm = prediction(prob.training.glm, train.cv$s33)
perf.glm = performance(pred.glm, measure="tpr", x.measure="fpr")

plot(perf.glm, col=2, lwd=3, main="ROC curve")
abline(0,1)

auc.glm = performance(pred.glm, "auc")@y.values
auc.glm


```

# Use a 10-fold cross-validation approach on the whole dat to estimate the test error rate\*

```{r, echo=T, warning=F}

cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)



model.glm.cv<-cv.glm(test.cv,model.glm,cost,10)
model.glm.cv$delta


```

low number of positive cases in the dataset may cause seemingly good performance. We can look at how well it performs on specific labels using a confusion matrix.

```{r,echo=T}
#as.factor(ifelse(sound == "33", "Yes","No")))


prob.training.glm = as.factor(ifelse(predict(model.glm, type="response")>0.5,"Yes","No"))
confusionMatrix(data = prob.training.glm, reference = train.cv$s33)


```

### Ridge

```{r, echo=T}
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))

train.neurons <- train.cv %>% select(V1:V34)
train.labels <- as.factor(train.cv$s33)


model.ridge=glmnet(train.neurons,train.labels,alpha=0,lambda=lambda.list.ridge,type.measure ='class', family = "binomial")


set.seed(1)
model.ridge.cv=cv.glmnet(as.matrix(train.neurons),train.labels, alpha = 0,nfolds = 5, lambda = lambda.list.ridge, family="binomial") 
lambdaSelect <- model.ridge.cv$lambda.min
plot(model.ridge.cv)
abline(v = log(lambdaSelect))

```

training MSE for the model corresponding to the optimal value of 'lambda' selected by the cross-validation above?

```{r, echo=T}
    
    model.ridge.cv$cvm[which (lambda.list.ridge==lambdaSelect)]

```

test MSE

```{r, echo=T}

pred.ridge=predict(model.ridge,s=lambdaSelect, newx = test.neurons, type="class") 
#mean((pred.ridge-test.labels)^2)



#pred.ridge <- predict.cv.glmnet(mymodel, newx = trainingdf, 
 #                        s = 'lambda.min', type = 'class')

confusionMatrix(data =as_factor(pred.ridge), reference = test.cv$s33) 
```

### Lasso

```{r, echo=T}
lambdaSelect = NULL

lambda.list.lasso = 1000 * exp(seq(0, log(1e-5), length = 100))

train.neurons <- train.cv %>% select(V1:V34)
train.labels <- as.factor(train.cv$s33)
test.neurons <- train.cv %>% select(V1:V34)

model.lasso=glmnet(train.neurons,train.labels,alpha=1,lambda=lambda.list.lasso,type.measure ='class', family = "binomial")


set.seed(1)
model.lasso.cv=cv.glmnet(as.matrix(train.neurons),train.labels, alpha = 1,nfolds = 10, lambda = lambda.list.lasso, family="binomial") 
lambdaSelect <- model.lasso.cv$lambda.min
plot(model.lasso.cv)
abline(v = log(lambdaSelect))
model.lasso.cv$cvm[which (lambda.list.ridge==lambdaSelect)]

```

```{r, echo=T}
pred.lasso=predict(model.lasso.cv,s=lambdaSelect, newx = test.neurons, type="class") 
#mean((pred.ridge-test.labels)^2)



#pred.ridge <- predict.cv.glmnet(mymodel, newx = trainingdf, 
 #                        s = 'lambda.min', type = 'class')

confusionMatrix(data =as_factor(pred.lasso), reference = test.cv$s33) 

```

## Trees

# Tree model

```{r,echo=T}

model.tree <- tree(s33 ~ ., data=train.cv)
summary(model.tree)
```

```{r,echo=T}
cv <- cv.tree(model.tree, K=10, FUN = prune.misclass)
best.cv = min(cv$size[cv$dev == min(cv$dev)])

model.tree.cv = prune.misclass (model.tree, best=best.cv)
draw.tree(model.tree.cv,nodeinfo = T)
text(model.tree.cv, pretty=0, col = "blue", cex = .5)
```

```{r,echo=T}
prob.training=NULL

prob.training.tree = predict(model.tree.cv, test.cv, type="class")


confusionMatrix(data = prob.training.tree, reference = test.cv$s33)
```

# Boosted trees to tackle the low number of positive cases to learn on

```{r,echo=T,warning=F}

set.seed(123)  # for reproducibility

model.gbm <- gbm(ifelse(s33=="Yes",1,0) ~ ., data = train.cv,  n.trees = 10000, shrinkage = 0.01)

prob.training.boost=NULL

prob.training.boost = as.factor(ifelse(predict(model.gbm, test.cv, type="response")>0.5,"Yes","No"))

confusionMatrix(data = prob.training.boost, reference = test.cv$s33)
```

# Random forest

```{r, echo=T}

model.rfor = randomForest(s33 ~ ., data = train.cv, ntree=500, strata=s33, importance=T)

model.rfor


#plot(model.rfor)
#legend("top", colnames(model.rfor$err.rate),col=1:4,cex=0.8,fill=1:4)

#model.rfor$importance

```

```{r,echo=T}



prob.training.boost = as.factor(ifelse(predict(model.gbm, test.cv, type="response")>0.05,"Yes","No"))
pred.boost = prediction(prob.training.boost, train.cv$s33)
perf.boost = performance(pred.glm, measure="tpr", x.measure="fpr")

plot(perf.boost, col=2, lwd=3, main="ROC curve")
abline(0,1)

auc.boost = performance(pred.boost, "auc")@y.values
auc.boost




plot(perfLog, col="red", lwd=3, main="ROC curve")
abline(0,1)

par(new=TRUE)

plot(perfPruned, col="blue", lwd=3, main="ROC curve")
abline(0,1)
par(new=TRUE)
plot(perfrFor, col="green", lwd=3, main="ROC curve")
abline(0,1)
par(new=TRUE)
plot(perfBoost, col="yellow", lwd=3, main="ROC curve")
abline(0,1)


legend("right","top" ,legend=c("ROC Logistic","ROC Pruned", "ROC random Foresr", "ROC Boosted"),
       col=c("red", "blue","green","yellow"),lty = 1)

```
