---
title: "Final Project"
author: "Filip Tomaska, Kelli Woodward"
subtitle: PSTAT 131/231
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: yes
  pdf_document:
    toc: yes
bibliography: references.bib
---

# Abstract

# Introduction

## Theoretical rationale

Brain tissue is organized into functionally and anatomically distinct regions. These specialized areas often perform very specific tasks such as handling of sensory information, calculate appropriate motor commands and many others. However, traditionally, the only detectable output other than the behavioral or physiological change itself is the variation in electrical properties of single neurons residing at the given site. The link between neuronal activity and its global effect is still very poorly understood. Since it is key to the understanding of how the information is coded and processed, it is subject to intense research. 

The classical approach towards studying brain areas handling sensory information is based on observing responses of single neurons to various changing parameters (Theunissen and Elie 2014). The collected data is subsequently translatable into neuron-specific receptive fields (RFs), study of which has revealed tonal organization of neurons both in the lower parts of the auditory pathway (mouse dorsal inferior colliculus (Barnstedt et al. 2015)) and in the auditory cortex (Issa et al. 2014)). The available data might suggest that the processing of the auditory input is dependent on single neurons encoding the perception of the input component that provokes them to maximal activity (Fregnac and Bathellier 2015). Furthermore, this hypothesis is in accord with the "neuronal doctrine" (Barlow 1972), which suggests such coding in other parts of the brain as well.

Although such linear coding of single input components is supported by data from lower levels of the auditory pathway (Andoni, Li and Pollak 2007), neurons from higher levels express significant non-linearity in their response patterns (Nelken 2004, Bathellier, Ushakova and Rumpel 2012, Deneux et al. 2016). For example, certain neurons specifically respond to unique physical attributes of natural sounds rather than being strictly frequency-tuned (Theunissen and Elie 2014). These non-linear responses are context-dependent (Kato, Gillet and Isaacson 2015), thus oppose the classic, exclusively feed-forward model and require the inclusion of input from local neuronal populations. 

In the light of growing evidence, it became apparent that the application of the classical receptive field concept in the auditory cortex should be revisited and possible alternatives need to be investigated. Another important factor was the appearance of novel techniques capable of recording large genetically defined neural populations,which has enabled in vivo testing of alternative models (namely two-photon imaging of genetically encoded calcium indicators (Lutcke and Helmchen 2011, Chen et al. 2013)). Unlike the classical electrophysiological approach, these are not limited to recordings of a small number of neurons, a limitation having strongly shaped the direction of scientific effort in the past.

The search for an alternative brought attention to a model already proposed for the hippocampal navigational apparatus. Here, specialized cell-types such as head direction cells or place cells encode spatial navigation and their activity is hypothesized to be organized following an attractor network model (Redish 1999, Zhang 1996). The attractor network model suggests that a specifically stimulated neuron excites cells of similar tuning (highly interconnected) while the lack activation results in the inhibition of cells responsive to different values of the given variable (due to a global inhibition net) (Redish 1999).

\
The activity of local cortical networks (of span \~200 µm) was discovered to be limited to very few reciprocally competitive response modes (mostly 1-3), while the shift between them is abrupt (Bathellier et al. 2012), thus expressing attractor-like dynamics. Such discrete shifting between few possible modes suggests, that the attractor-like dynamics could form a base for the encoding of perceptual categories (Wang 2008). Moreover, the role of the auditory cortex has recently been hypothesized to lie within a more complex processing of the auditory input than previously thought, namely representing sounds as auditory objects (Russ, Lee and Cohen 2007). Since the sensory categorization offers a balanced coding strategy between detail acquisition and generalization across experiences (Seger and Miller 2010), it reasonably fits within the current context of the auditory cortex.

In summary, the investigation of discrete network dynamics and categorization in the mouse auditory cortex by Bathellier et al. (Bathellier et al. 2012) has produced a significant amount of useful information. Among these is evidence supporting the idea that it is not single neurons but hundreds of cells, which form a functional unit in the neocortex (Averbeck, Latham and Pouget 2006). Moreover, it was shown that single subnetworks are comprised of a redundant number of cells and are non-discretely divided, possibly to counter the high overall noise. However, most importantly, it was shown that different response modes (of a local neuronal network) recorded by calcium imaging reflect the categorization of sounds observed also behaviorally, thus suggesting that the attractor final states may be directly representing the perceptual category. 

\
\
\

```{r library loading, error=F, message=FALSE, warning=FALSE, echo=T}
library(R.matlab)
library(readr)
library(tidyverse)
library(data.table)
library(cluster)
library(dendextend)
library(factoextra)
library(ROCR)
library(boot)
library(caret)
library(glmnet) 
library(tree) 
library(maptree) 
library(randomForest) 
library(gbm) 
library(e1071)
library(SparseM)
```

# Pre-processing

## Data loading

Some of the functional analysis had to be performed in MATLAB since the Two Photon Processor [@tomek2013] , the framework used for raw fluorescence trace analysis, does not exist outside MATLAB. Consequently, data in the form of a MATLAB structured list is imported and transformed into a tibble dataframe

```{r data import, error=F, warning=FALSE, echo=T, cache=TRUE}

dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f19<-dataMat$sortAudFile[1]
f33<-dataMat$sortAudFile[2]
f34a<-dataMat$sortAudFile[3]
f34b <-dataMat$sortAudFile[4]
f34c <-dataMat$sortAudFile[5]
f35  <-dataMat$sortAudFile[6]
f37  <-dataMat$sortAudFile[7]
f38 <-dataMat$sortAudFile[8]
f40 <-dataMat$sortAudFile[9]
f41a <-dataMat$sortAudFile[10]
f41b <-dataMat$sortAudFile[11]
f42  <-dataMat$sortAudFile[12]
f43a <-dataMat$sortAudFile[13]
f43b <-dataMat$sortAudFile[14]
f44  <-dataMat$sortAudFile[15]

df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)

tempF = f34c[[1]]

for (j in c(1:8)){
  for (i in c(1:39)) {
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]),sound = i,day = j-1))
  }
  }
  
df$sound = factor(df$sound)
df$day = factor(df$day)
df.Day <-  df%>% filter(df$day==0)
df.neurons <-  df.Day%>% select(V1:V34)
```

# Exploratory

```{r, echo=T, warning=FALSE}

sumVect <- c(1:39)
  
for (i in c(1:39)) {
  df.sound = NULL
  df.sound <-  df%>% filter(df$sound==i)
  
  temp.df<- as.matrix(df.sound%>%select(V1:V34))

  sumVect[i] = sum(colSums(temp.df))
 
  sortSum = sort(sumVect, decreasing = TRUE)
  
  which(sumVect==(head(sortSum)[1]))
   
}
```

## hierarchical clustering

The hypothesis proposed by the work inspiring these experiments postulates that local neuronal populations represent sensory precepts by categorizing them into a small number of perceptual categories [@bathellier2012] Since our aim was to reproduce these results using tools enabling long-term imaging, we first try to identify sounds producing a recognizable activity pattern.

In order to do this, we employ an unsupervised learning method, **hierarchical clustering**.

We perform this on a data subset from the first day of recordings, since that is when fields of view were selected for long term studies by performing clustering online during imaging.

### Randomized sampling

In order to assess the relevance of hierarchical clustering performed on our data, we first try to analyze a randomized dataset.

Finding clusters of similar activity patterns within a **randomized** (random sampling of sounds with replacement)dataset, might indicate that any clusters we observe in the actual data might be, in fact, an artifact arising from the nature of the data.

## Resampling to show data is not random

```{r,echo=T}
#reloading data

f34c = NULL
df = NULL
dataMat = NULL

dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f34c <-dataMat$sortAudFile[5]
df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)
tempF = f34c[[1]]

#random sampling, scrambling sound identity, with replacement

set.seed(123)

for (j in c(1:8)){
  for (i in c(2:39)) {
    sound.random = sample(c(1:39),1)
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]), sound = sound.random, day = j-1))
  }
  }
  
df$sound = factor(df$sound)
df$day = factor(df$day)
df.Day <-  df%>% filter(df$day==0)
df.neurons <-  df.Day%>% select(V1:V34)

#clustering
dist_mat <- dist(df.neurons,method = "euclidian")
hclust_avg <- hclust(dist_mat, method = "complete")
cut_avg <- cutree(hclust_avg, k = 2)
table(cut_avg, df.Day$sound)
```

```{r,echo=T}


table.accum <- matrix(, nrow=2,ncol=8)
maxSecond <- matrix(, ncol=8)

for (k in c(0:7)){
  
  df.Day <-  df%>% filter(df$day==k)


df.neurons <-  df.Day%>% select(V1:V34)
dist_mat <- dist(df.neurons,method = "euclidian")


hclust_avg <- hclust(dist_mat, method = "average")
cut_avg <- cutree(hclust_avg, k = 2)


table.accum[,k+1] = table(cut_avg, df.Day$sound)[,33]
maxSecond[k+1] = as.numeric(which.max(table(cut_avg, df.Day$sound)[2,]))

}

#Display the counts of 
table.accum
maxSecond
```

We can observe

```{r, error=F, warning=FALSE, echo=T, cache=TRUE}

#reloading data again for proper clustering

f34c = NULL
df = NULL


dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f34c <-dataMat$sortAudFile[5]

df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)

tempF = f34c[[1]]

for (j in c(1:8)){
  for (i in c(1:39)) {
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]),sound = i,day = j-1))
  }
  }
  
df$sound = factor(df$sound)
df$day = factor(df$day)
df.Day <-  df%>% filter(df$day==0)
df.neurons <-  df.Day%>% select(V1:V34)

```

### Looking at data day by day to determine which sound gets clustered in the second cluster the most

```{r,echo=T}


table.accum <- matrix(, nrow=2,ncol=8)
maxSecond <- matrix(, ncol=8)

for (k in c(0:7)){
  
  df.Day <-  df%>% filter(df$day==k)


df.neurons <-  df.Day%>% select(V1:V34)
dist_mat <- dist(df.neurons,method = "euclidian")


hclust_avg <- hclust(dist_mat, method = "average")
cut_avg <- cutree(hclust_avg, k = 2)


table.accum[,k+1] = table(cut_avg, df.Day$sound)[,33]
maxSecond[k+1] = as.numeric(which.max(table(cut_avg, df.Day$sound)[2,]))

}

#Display the counts of 
maxSecond
```

#### Conclusion

The identities of sounds and their distribution are different in a randomized sample, thus our recordings are likely not an artifact.

## PCA analysis

In order to assess correlations between variables (neurons #1 to #38) we perform *principal component analysis*.

```{r, echo=T}

pca.neurons = prcomp(df.neurons, scale=TRUE, center = TRUE)

pr.var <- (pca.neurons$sdev)^2
pve=pr.var/sum(pr.var)

 plot(pve, xlab="Principal Component",
     ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
 
  plot(cumsum(pve), xlab="Principal Component ",
     ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')

abline(v =min(which((cumsum(pve)>=0.9))))

```

#### Conclusion PCA

We observe that in order to explain 90% of total variation in the data, we need at least **27** principal components. The high number of components suggests we indeed do need all of our variables in order to capture the variance, thus we should use all of them in our future analysis

# Supervised learning

## Data preparation

First, we need to define a factor variable defining whether the observation was obtained as a response to the examined sound. Based on the hierarchical clustering performed earlier, this sound is sound **33**,

```{r, include=FALSE}

df = df %>%
  mutate(s33=as.factor(ifelse(sound == "33", "Yes","No")))

```

#### Split

Since only one sound is considered a positive case, our data contains a small fraction of positive cases for learning and training purposes. For that reason, it is important to perform stratified sampling, so that the limited number of positive labels is evenly split between training and testing data chunks,

```{r, include=FALSE}

#stratified

set.seed(123)
df <- df %>% mutate(id = row_number())
#Check IDs
head(df$id)
train.index <- createDataPartition(df$s33, p = .7, list = FALSE)

train <- df[ train.index,]
test  <- df[-train.index,]
length(which(test$s33=="Yes"))
```

## Model fitting

### Logistic regression

```{r certain values, echo=T, message=FALSE, warning=FALSE}
#generate subdatasets of only activity data for learning purposes
excl<-c('sound','day','id')
train.cv <- train %>% select(., -excl)
test.cv <- test %>% select(., -excl)
```

```{r GLM fit, echo=T, message=FALSE, warning=FALSE}
model.glm = glm(formula = s33~., data=train.cv, family = binomial)
#summary(model.glm)

```

In order to calculate the error rate, we use the function we were given as a part of homework assignment

```{r error rate def, message=FALSE, warning=FALSE, ,echo=T}
calc_error_rate <- function(predicted.value, true.value){
  return(mean(true.value!=predicted.value)) }
```

#### Training and Testing error rate for GLM

Minor formatting of prediction output is needed to obtain a training and testing error

```{r glm errors, message=FALSE, warning=FALSE, echo=T}
prob.training.glm = NULL
prob.training.glm = as.factor(ifelse(predict(model.glm, type="response")>0.5,"Yes","No"))

trainError<- calc_error_rate(prob.training.glm,train$s33)
trainError
testError<- calc_error_rate(prob.training.glm,test$s33)
testError
```

**Conclusion:**

According to these rates, the model classifies a given observation in **\~3%** of the cases.

The *testing error* is slightly higher than the *training error,* which can be expected due to the fact that none of the data in the **test** split were available for training.

#### Plotting the ROC curve and computing AUC

```{r ROC glm, message=FALSE, warning=FALSE, echo=TRUE}
prob.training.glm = NULL
prob.training.glm =predict(model.glm)
pred.glm = prediction(prob.training.glm, train.cv$s33)
perf.glm = performance(pred.glm, measure="tpr", x.measure="fpr")

plot(perf.glm, col=2, lwd=3, main="ROC curve")
abline(0,1)

auc.glm = performance(pred.glm, "auc")@y.values
auc.glm
```

**Conclusion:**

The performance of the model as assessed by the *Area under curve* metric seems to be fairly good

However, since the positive label is so sparse in our data, we should investigate more options of performance evaluation, starting with 10 fold cross-validation of the whole dataset.

#### Cross-validation using the whole dataset for a better estimate of the test error.

```{r cv glm, echo=T, warning=F}

cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)

model.glm.cv<-cv.glm(test.cv,model.glm,cost,10)
model.glm.cv$delta


```

10-fold cross validation has produced similar error rates. However, this does not necessarily solve the problem of having too little positive cases (meaning cases when an observation belongs to sound 33). This metric might be very heavily skewed by the fact that classifying a non33 observation correctly also results in a true positive.

Thus, we need should investigate what the rates are in between groups.

#### Confusion matrix

Constructing a confusion matrix should provide the most insight into how well the model can classify whether an observation was or was not elicited by sound 33.

```{r conf matrix glm,echo=T}
#as.factor(ifelse(sound == "33", "Yes","No")))


prob.training.glm = as.factor(ifelse(predict(model.glm, type="response")>0.5,"Yes","No"))
confusionMatrix(data = prob.training.glm, reference = train.cv$s33)


```

**Conclusion**

Although the overall accuracy of the model is high, it is clear that it's accuracy in terms of correctly predicting the YES case in sound33 variable is fairly **poor**.

### Ridge

```{r eval=FALSE, include=FALSE}
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))

train.neurons <- train.cv %>% select(V1:V34)
train.labels <- as.factor(train.cv$s33)


#model.ridge=glmnet(train.neurons,train.labels,alpha=0,lambda=lambda.list.ridge,type.measure ='class', family = "binomial")


set.seed(1)
model.ridge.cv=cv.glmnet(as.matrix(train.neurons),train.labels, alpha = 0,nfolds = 5, lambda = lambda.list.ridge, family="binomial", type.measure ='class') 
lambdaSelect <- model.ridge.cv$lambda.min
plot(model.ridge.cv)
abline(v = log(lambdaSelect))

```

training MSE for the model corresponding to the optimal value of 'lambda' selected by the cross-validation above?

```{r eval=FALSE, include=FALSE}
    
    model.ridge.cv$cvm[which (lambda.list.ridge==lambdaSelect)]

```

test MSE

```{r eval=FALSE, include=FALSE}

pred.ridge=predict(model.ridge,s=lambdaSelect, newx = test.neurons, type="class") 
#mean((pred.ridge-test.labels)^2)



#pred.ridge <- predict.cv.glmnet(mymodel, newx = trainingdf, 
 #                        s = 'lambda.min', type = 'class')

confusionMatrix(data =as_factor(pred.ridge), reference = test.cv$s33) 
```

### Lasso

```{r eval=FALSE, include=FALSE}
lambdaSelect = NULL

lambda.list.lasso = 1000 * exp(seq(0, log(1e-5), length = 100))

train.neurons <- train.cv %>% select(V1:V34)
train.labels <- as.factor(train.cv$s33)
test.neurons <- train.cv %>% select(V1:V34)

model.lasso=glmnet(train.neurons,train.labels,alpha=1,lambda=lambda.list.lasso,type.measure ='class', family = "binomial")


set.seed(1)
model.lasso.cv=cv.glmnet(as.matrix(train.neurons),train.labels, alpha = 1,nfolds = 10, lambda = lambda.list.lasso, family="binomial") 
lambdaSelect <- model.lasso.cv$lambda.min
plot(model.lasso.cv)
abline(v = log(lambdaSelect))
model.lasso.cv$cvm[which (lambda.list.ridge==lambdaSelect)]

```

```{r eval=FALSE, include=FALSE}
pred.lasso=predict(model.lasso.cv,s=lambdaSelect, newx = test.neurons, type="class") 
#mean((pred.ridge-test.labels)^2)



#pred.ridge <- predict.cv.glmnet(mymodel, newx = trainingdf, 
 #                        s = 'lambda.min', type = 'class')

confusionMatrix(data =as_factor(pred.lasso), reference = test.cv$s33) 

```

## Tree models

```{r basic tree, message=FALSE, warning=FALSE, echo=T}

model.tree <- tree(s33 ~ ., data=train.cv)
summary(model.tree)

```

```{r,echo=T, eval=FALSE, include=FALSE}
cv <- cv.tree(model.tree, K=10, FUN = prune.misclass)
best.cv = min(cv$size[cv$dev == min(cv$dev)])

model.tree.cv = prune.misclass (model.tree, best=best.cv)
draw.tree(model.tree.cv,nodeinfo = T)
text(model.tree.cv, pretty=0, col = "blue", cex = .5)
```

```{r,echo=T}
prob.training=NULL

prob.training.tree = predict(model.tree, test.cv, type="class")


confusionMatrix(data = prob.training.tree, reference = test.cv$s33)
```

#### Boosted trees to tackle the low number of positive cases to learn on

```{r,echo=T,warning=F}

set.seed(123)  # for reproducibility

model.gbm <- gbm(ifelse(s33=="Yes",1,0) ~ ., data = train.cv,  n.trees = 10000, shrinkage = 0.01)

prob.training.boost=NULL

prob.training.boost = as.factor(ifelse(predict(model.gbm, test.cv, type="response")>0.5,"Yes","No"))

confusionMatrix(data = prob.training.boost, reference = test.cv$s33)
```

#### Random forest

```{r, echo=T}

model.rfor = randomForest(s33 ~ ., data = train.cv, ntree=500, strata=s33, importance=T)

model.rfor


#plot(model.rfor)
#legend("top", colnames(model.rfor$err.rate),col=1:4,cex=0.8,fill=1:4)

#model.rfor$importance

```

### Support Vector Machines

```{r}

model.svm = svm(formula = s33 ~ .,
                 data = train.cv,
                 type = 'C-classification',
                 kernel = 'polynomial',
                degree = 4,
                scale = TRUE,
                cross = 100,
                tolerance = 0.01,
                cost = 10,
                shrinking = FALSE
                )

```

```{r}
pred.svm = predict(model.svm, newdata = test.cv)

confusionMatrix(data = pred.svm, reference = test.cv$s33)
```
