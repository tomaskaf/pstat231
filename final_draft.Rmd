---
title: "Final Project"
author: "Filip Tomaska, Kelli Woodward"
subtitle: PSTAT 131/231
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: yes
  pdf_document:
    toc: yes
---


#Abstract

Neuronal activity can be recorded using various recording techniques. Optophysiology provides an option using genetically encoded fluorescent reporters, which generate optical readout of neuronal activity.


# Introduction



```{r library loading, error=F, message=FALSE, warning=FALSE, echo=T}
library(R.matlab)
library(readr)
library(tidyverse)
library(data.table)
library(cluster)
library(dendextend)
library(factoextra)
library(ROCR)
library(boot)
library(caret)
library(glmnet) 
library(tree) 
library(maptree) 
library(randomForest) 
library(gbm) 
```


# Pre-processing

## Data loading

```{r data import, error=F, warning=FALSE, echo=T, cache=TRUE}

dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f19<-dataMat$sortAudFile[1]
f33<-dataMat$sortAudFile[2]
f34a<-dataMat$sortAudFile[3]
f34b <-dataMat$sortAudFile[4]
f34c <-dataMat$sortAudFile[5]
f35  <-dataMat$sortAudFile[6]
f37  <-dataMat$sortAudFile[7]
f38 <-dataMat$sortAudFile[8]
f40 <-dataMat$sortAudFile[9]
f41a <-dataMat$sortAudFile[10]
f41b <-dataMat$sortAudFile[11]
f42  <-dataMat$sortAudFile[12]
f43a <-dataMat$sortAudFile[13]
f43b <-dataMat$sortAudFile[14]
f44  <-dataMat$sortAudFile[15]

df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)

tempF = f34c[[1]]

for (j in c(1:8)){
  for (i in c(1:39)) {
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]),sound = i,day = j-1))
  }
  }
  

df$sound = factor(df$sound)
df$day = factor(df$day)



df.Day <-  df%>% filter(df$day==0)


df.neurons <-  df.Day%>% select(V1:V34)

```


# Exploratory

```{r, echo=T, warning=FALSE}

sumVect <- c(1:39)
  
for (i in c(1:39)) {
  df.sound = NULL
  df.sound <-  df%>% filter(df$sound==i)
  
  temp.df<- as.matrix(df.sound%>%select(V1:V34))

  sumVect[i] = sum(colSums(temp.df))
 
  sortSum = sort(sumVect, decreasing = TRUE)
  
  which(sumVect==(head(sortSum)[1]))
   
}


```



## hierarchical clustering

In order to


```{r Hierachical clustering, echo=T, warning=FALSE}

dist_mat <- dist(df.neurons,method = "euclidian")

fviz_dist(dist_mat, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
## looking at all days through a loop one by one

```{r,echo=T, warning=FALSE}

table.accum <- matrix(, nrow=2,ncol=8)
maxSecond <- matrix(, ncol=8)

for (k in c(0:7)){
  
  df.Day <-  df%>% filter(df$day==k)


df.neurons <-  df.Day%>% select(V1:V34)
dist_mat <- dist(df.neurons,method = "euclidian")


hclust_avg <- hclust(dist_mat, method = "average")
cut_avg <- cutree(hclust_avg, k = 2)




table.accum[,k+1] = table(cut_avg, df.Day$sound)[,34]
maxSecond[k+1] = as.numeric(which.max(table(cut_avg, df.Day$sound)[2,]))

}

table.accum
maxSecond
```


## looking at all days in one big dataset


```{r hclust with all days, warning=FALSE, echo=T}



df.neurons <-  df%>% select(V1:V34)
dist_mat <- dist(df.neurons,method = "euclidian")
#fviz_dist(dist_mat, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


hclust_avg <- hclust(dist_mat, method = "complete")
cut_avg <- cutree(hclust_avg, k = 2)

## dendrogram: branches colored by 3 groups
dend1 = as.dendrogram(hclust_avg)

# color branches and labels by 3 clusters 
dend1 = color_branches(dend1, k=2)
dend1 = color_labels(dend1, k=2)
# change label size
dend1 = set(dend1, "labels_cex", 0.3) 
plot(dend1, horiz=T, main = "Dendrogram colored by three clusters")
cut_avg <- cutree(hclust_avg, k = 2)

table(cut_avg, df$sound)


```

## Resampling to show data is not random

```{r,echo=T}


dataMat <- R.matlab::readMat("sortAudFile.mat",fixNames=T, ) 
f19<-dataMat$sortAudFile[1]
f33<-dataMat$sortAudFile[2]
f34a<-dataMat$sortAudFile[3]
f34b <-dataMat$sortAudFile[4]
f34c <-dataMat$sortAudFile[5]
f35  <-dataMat$sortAudFile[6]
f37  <-dataMat$sortAudFile[7]
f38 <-dataMat$sortAudFile[8]
f40 <-dataMat$sortAudFile[9]
f41a <-dataMat$sortAudFile[10]
f41b <-dataMat$sortAudFile[11]
f42  <-dataMat$sortAudFile[12]
f43a <-dataMat$sortAudFile[13]
f43b <-dataMat$sortAudFile[14]
f44  <-dataMat$sortAudFile[15]

df<-as_tibble(f34c[[1]][[1]][[1]])

df = df %>% add_column(sound = 1)
df = df %>% add_column(day = 0)

tempF = f34c[[1]]

for (j in c(1:8)){
  for (i in c(2:39)) {
    sound.random = sample(c(1:39),1)
   df = df %>% add_row(tibble(as_tibble(tempF[i][[1]][[j]]), sound = sound.random, day = j-1))
  }
  }
  

df$sound = factor(df$sound)
df$day = factor(df$day)



df.Day <-  df%>% filter(df$day==0)


df.neurons <-  df.Day%>% select(V1:V34)


dist_mat <- dist(df.neurons,method = "euclidian")

fviz_dist(dist_mat, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
hclust_avg <- hclust(dist_mat, method = "complete")

cut_avg <- cutree(hclust_avg, k = 2)

table(cut_avg, df.Day$sound)

```


## lets try PCA

```{r, echo=T}

pca.neurons = prcomp(df.neurons, scale=TRUE)

pr.var <- (pca.neurons$sdev)^2
pve=pr.var/sum(pr.var)

 plot(pve, xlab="Principal Component",
     ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
 
  plot(cumsum(pve), xlab="Principal Component ",
     ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')

abline(v =min(which((cumsum(pve)>=0.9))))

```

# Supervised learning

## Add a Factor variable 

```{r, include=FALSE}

df = df %>%
  mutate(s33=as.factor(ifelse(sound == "33", "Yes","No")))

```


## Split


```{r, include=FALSE}

#stratified

set.seed(123)
df <- df %>% mutate(id = row_number())
#Check IDs
head(df$id)
train.index <- createDataPartition(df$s33, p = .7, list = FALSE)

train <- df[ train.index,]
test  <- df[-train.index,]
length(which(test$s33=="Yes"))
```


```{r eval=FALSE, include=FALSE}


#nonstratified

df <- df %>% mutate(id = row_number())
#Check IDs
head(df$id)

#Create training set
train <- df %>% sample_frac(.70)
#Create test set
test  <- anti_join(df, train, by = 'id')
length(which(test$s33=="Yes"))
```


## logistic regression

```{r, echo=T}
#generate subdatasets of only activity data for learning purposes
excl<-c('sound','day','id')
train.cv <- train %>% select(., -excl)
test.cv <- test %>% select(., -excl)
```

```{r, echo=T}
model.glm = glm(formula = s33~., data=train.cv, family = binomial)
#summary(model.glm)

```

```{r ,echo=T,message=FALSE,warning=FALSE}
calc_error_rate <- function(predicted.value, true.value){
  return(mean(true.value!=predicted.value)) }
```

```{r,echo=TRUE}



#as.factor(ifelse(sound == "33", "Yes","No")))

```

```{r,echo=T,message=FALSE,warning=FALSE}
prob.training.glm = NULL
prob.training.glm = as.factor(ifelse(predict(model.glm, type="response")>0.5,"Yes","No"))

trainError<- calc_error_rate(prob.training.glm,train$s33)
trainError
testError<- calc_error_rate(prob.training.glm,test$s33)
testError



prob.training.glm = NULL
prob.training.glm =predict(model.glm)
pred.glm = prediction(prob.training.glm, train.cv$s33)
perf.glm = performance(pred.glm, measure="tpr", x.measure="fpr")

plot(perf.glm, col=2, lwd=3, main="ROC curve")
abline(0,1)

auc.glm = performance(pred.glm, "auc")@y.values
auc.glm


```

# Use a 10-fold cross-validation approach on the whole dat to estimate the test error rate*

```{r, echo=T, warning=F}

cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)



model.glm.cv<-cv.glm(test.cv,model.glm,cost,10)
model.glm.cv$delta


```


low number of positive cases in the dataset may cause seemingly good performance. We can look at how well it performs on specific labels using a confusion matrix.


```{r,echo=T}
#as.factor(ifelse(sound == "33", "Yes","No")))


prob.training.glm = as.factor(ifelse(predict(model.glm, type="response")>0.5,"Yes","No"))
confusionMatrix(data = prob.training.glm, reference = train.cv$s33)


```
## Ridge

```{r, echo=T}
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))

train.neurons <- train.cv %>% select(V1:V34)
train.labels <- as.factor(train.cv$s33)


model.ridge=glmnet(train.neurons,train.labels,alpha=0,lambda=lambda.list.ridge,type.measure ='class', family = "binomial")


set.seed(1)
model.ridge.cv=cv.glmnet(as.matrix(train.neurons),train.labels, alpha = 0,nfolds = 5, lambda = lambda.list.ridge, family="binomial") 
lambdaSelect <- model.ridge.cv$lambda.min
plot(model.ridge.cv)
abline(v = log(lambdaSelect))

```
training MSE for the model corresponding to the optimal value of 'lambda' selected by the cross-validation above?
```{r, echo=T}
    
    model.ridge.cv$cvm[which (lambda.list.ridge==lambdaSelect)]

```
test MSE

```{r, echo=T}

pred.ridge=predict(model.ridge,s=lambdaSelect, newx = test.neurons, type="class") 
#mean((pred.ridge-test.labels)^2)



#pred.ridge <- predict.cv.glmnet(mymodel, newx = trainingdf, 
 #                        s = 'lambda.min', type = 'class')

confusionMatrix(data =as_factor(pred.ridge), reference = test.cv$s33) 
```

# Lasso

```{r, echo=T}
lambdaSelect = NULL

lambda.list.lasso = 1000 * exp(seq(0, log(1e-5), length = 100))

train.neurons <- train.cv %>% select(V1:V34)
train.labels <- as.factor(train.cv$s33)
test.neurons <- train.cv %>% select(V1:V34)

model.lasso=glmnet(train.neurons,train.labels,alpha=1,lambda=lambda.list.lasso,type.measure ='class', family = "binomial")


set.seed(1)
model.lasso.cv=cv.glmnet(as.matrix(train.neurons),train.labels, alpha = 1,nfolds = 10, lambda = lambda.list.lasso, family="binomial") 
lambdaSelect <- model.lasso.cv$lambda.min
plot(model.lasso.cv)
abline(v = log(lambdaSelect))
model.lasso.cv$cvm[which (lambda.list.ridge==lambdaSelect)]

```
```{r, echo=T}
pred.lasso=predict(model.lasso.cv,s=lambdaSelect, newx = test.neurons, type="class") 
#mean((pred.ridge-test.labels)^2)



#pred.ridge <- predict.cv.glmnet(mymodel, newx = trainingdf, 
 #                        s = 'lambda.min', type = 'class')

confusionMatrix(data =as_factor(pred.lasso), reference = test.cv$s33) 

```

## Trees

# Tree model

```{r,echo=T}

model.tree <- tree(s33 ~ ., data=train.cv)
summary(model.tree)
```


```{r,echo=T}
cv <- cv.tree(model.tree, K=10, FUN = prune.misclass)
best.cv = min(cv$size[cv$dev == min(cv$dev)])

model.tree.cv = prune.misclass (model.tree, best=best.cv)
draw.tree(model.tree.cv,nodeinfo = T)
text(model.tree.cv, pretty=0, col = "blue", cex = .5)
```

```{r,echo=T}
prob.training=NULL

prob.training.tree = predict(model.tree.cv, test.cv, type="class")


confusionMatrix(data = prob.training.tree, reference = test.cv$s33)
```

# Boosted trees to tackle the low number of positive cases to learn on


```{r,echo=T,warning=F}

set.seed(123)  # for reproducibility

model.gbm <- gbm(ifelse(s33=="Yes",1,0) ~ ., data = train.cv,  n.trees = 10000, shrinkage = 0.01)

prob.training.boost=NULL

prob.training.boost = as.factor(ifelse(predict(model.gbm, test.cv, type="response")>0.5,"Yes","No"))

confusionMatrix(data = prob.training.boost, reference = test.cv$s33)
```

# Random forest

``` {r, echo=T}

model.rfor = randomForest(s33 ~ ., data = train.cv, ntree=500, strata=s33, importance=T)

model.rfor


#plot(model.rfor)
#legend("top", colnames(model.rfor$err.rate),col=1:4,cex=0.8,fill=1:4)

#model.rfor$importance

```

```{r,echo=T}



prob.training.boost = as.factor(ifelse(predict(model.gbm, test.cv, type="response")>0.05,"Yes","No"))
pred.boost = prediction(prob.training.boost, train.cv$s33)
perf.boost = performance(pred.glm, measure="tpr", x.measure="fpr")

plot(perf.boost, col=2, lwd=3, main="ROC curve")
abline(0,1)

auc.boost = performance(pred.boost, "auc")@y.values
auc.boost




plot(perfLog, col="red", lwd=3, main="ROC curve")
abline(0,1)

par(new=TRUE)

plot(perfPruned, col="blue", lwd=3, main="ROC curve")
abline(0,1)
par(new=TRUE)
plot(perfrFor, col="green", lwd=3, main="ROC curve")
abline(0,1)
par(new=TRUE)
plot(perfBoost, col="yellow", lwd=3, main="ROC curve")
abline(0,1)


legend("right","top" ,legend=c("ROC Logistic","ROC Pruned", "ROC random Foresr", "ROC Boosted"),
       col=c("red", "blue","green","yellow"),lty = 1)

```

