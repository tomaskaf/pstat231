---
title: "Homework 3"
author: "Filip Tomaska"
subtitle: PSTAT 131/231
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: yes
  pdf_document:
    toc: yes
---




    
```{r, include=FALSE}
#Load the required libraries:
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) 
library(ISLR) 
library(glmnet) 
library(tree) 
library(maptree) 
library(randomForest) 
library(gbm) 
library(ROCR)
```

# Predicting carseats sales using regularized regression methods


```{r, include=FALSE}
Carseats <- ISLR::Carseats 
```

```{r, include=FALSE}

set.seed(123)

dat <- model.matrix(Sales~., Carseats) 

train = sample(nrow(dat), 30)
x.train = dat[train, ]
y.train = Carseats[train, ]$Sales

# The rest as test data
x.test = dat[-train, ]
y.test = Carseats[-train, ]$Sales

```

## 1a Fit a ridge regression model to the training set to predict Sales using all other variables as predictors. Use the built-in cross-validation in cv.glmnet to choose the optimal value of tuning parameter 'lambda' from the following list of 'lambda' values using a 5-fold CV. 


```{r, echo=T}
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))

ridgeModel=glmnet(x.train,y.train,alpha=0,lambda=lambda.list.ridge)


set.seed(1)
ridgeModel.CV=cv.glmnet(x.train, y.train, alpha = 0,nfolds = 5, lambda = lambda.list.ridge) 
lambdaSelect <- ridgeModel.CV$lambda.min
plot(ridgeModel.CV)
abline(v = log(lambdaSelect))

```


## What is the training MSE for the model corresponding to the optimal value of 'lambda' selected by the cross-validation above?

```{r, echo=T}
    
    ridgeModel.CV$cvm[which (lambda.list.ridge==lambdaSelect)]

```


## What is the test MSE for that same model?


```{r, echo=T}
   
 ridgeModel.pred=predict(ridgeModel,s=lambdaSelect, newx = x.test) 
  mean((ridgeModel.pred-y.test)^2)
 
```


## 1b Report the ridge coefficient

```{r, echo=T}
    
coef(ridgeModel)[,(which(lambda.list.ridge==lambdaSelect))]

```

## 2a Fit a lasso model to the training set to predict Sales using all other variables as predictors. Use the built-in cross-validation in cv.glmnet to choose the optimal value of tuning parameter ⁄ from the following list of lambda values using a 10-fold CV


```{r, echo=T}
lambda.list.lasso = 2 * exp(seq(0, log(1e-4), length = 100))

lassoModel=glmnet(x.train,y.train,alpha=1,lambda=lambda.list.lasso)


set.seed(1)
lassoModel.CV=cv.glmnet(x.train, y.train, alpha = 1,nfolds = 10, lambda = lambda.list.lasso) 
lambdaSelect <- lassoModel.CV$lambda.min
plot(lassoModel.CV)
abline(v = log(lambdaSelect))

```

## Report the lasso coefficients

```{r, echo=T}
coef(lassoModel)[,(which(lambda.list.lasso==lambdaSelect))]
```

## What is the training MSE for the model corresponding to the optimal value of 'lambda' selected by the cross-validation above?

```{r, echo=T}
    
    lassoModel.CV$cvm[which (lambda.list.lasso==lambdaSelect)]

```


## What is the test MSE for that same model?

```{r, echo=T}
   
 lassoModel.pred=predict(lassoModel,s=lambdaSelect, newx = x.test) 
  mean((lassoModel.pred-y.test)^2)
 
```


## What is the test MSE for that same model?

```{r, include=FALSE}
   
coef(lassoModel)[,(which(lambda.list.lasso==lambdaSelect))]
 
```

```{r, include=FALSE}
drug <- read_csv('drug.csv',
                col_names=c('ID','Age','Gender','Education','Country',
                'Ethnicity','Nscore',
                'Escore','Oscore','Ascore','Cscore',
                'Impulsive','SS','Alcohol','Amphet','Amyl','Benzos',
                'Caff','Cannabis', 'Choc','Coke','Crack','Ecstasy',
                'Heroin','Ketamine','Legalh','LSD','Meth',
                'Mushrooms','Nicotine','Semer','VSA'))
```

Define a new factor response variable recent_cannabis_use which is “Yes” if a person has used cannabis
within a year, and “No” otherwise.
